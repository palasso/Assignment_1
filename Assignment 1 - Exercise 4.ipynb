{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, TweetTokenizer #Tokenizing sentences\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"europarl-v7-en.txt\", encoding = \"utf8\") as file:\n",
    "    source = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split source into training / development / test set - 70/10/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sents = sent_tokenize(source)\n",
    "sents_len=len(source_sents)\n",
    "train_sents=source_sents[0:round(sents_len*0.7)]\n",
    "development_sents=source_sents[round(sents_len*0.7):round(sents_len*0.8)]\n",
    "test_sents=source_sents[round(sents_len*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sets by Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_wt = TweetTokenizer()\n",
    "\n",
    "train_tokens = tweet_wt.tokenize(' '.join(train_sents))\n",
    "development_tokens = tweet_wt.tokenize(' '.join(development_sents))\n",
    "test_tokens = tweet_wt.tokenize(' '.join(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding tokens appearing in the training test rarely... Replacing those with \\*UKN\\* in all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(train_tokens)\n",
    "frequent_tokens=set([k for k,v in count.items() if v >= 10])\n",
    "train_tokens=[train_token if (train_token in frequent_tokens) else \"*UNK*\" for train_token in train_tokens]\n",
    "development_tokens=[development_token if (development_token in frequent_tokens) else \"*UNK*\" for development_token in development_tokens]\n",
    "test_tokens=[test_token if (test_token in frequent_tokens) else \"*UNK*\" for test_token in test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sets by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_tokenized = []\n",
    "development_sents_tokenized = []\n",
    "test_sents_tokenized = []\n",
    "\n",
    "for sent in train_sents:\n",
    "    sent_tmp = tweet_wt.tokenize(sent)\n",
    "    sent = [word if (word in frequent_tokens) else \"*UNK*\" for word in sent_tmp]\n",
    "    train_sents_tokenized.append(sent)\n",
    "for sent in development_sents:\n",
    "    sent_tmp = tweet_wt.tokenize(sent)\n",
    "    sent = [word if (word in frequent_tokens) else \"*UNK*\" for word in sent_tmp]\n",
    "    development_sents_tokenized.append(sent)\n",
    "for sent in test_sents:\n",
    "    sent_tmp = tweet_wt.tokenize(sent)\n",
    "    sent = [word if (word in frequent_tokens) else \"*UNK*\" for word in sent_tmp]\n",
    "    test_sents_tokenized.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tokens into Bigram and Trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()\n",
    "\n",
    "for sent in train_sents_tokenized:\n",
    "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    \n",
    "unigram_counter[('*start*',)] = len(train_sents_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring ngram methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set(train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_bigram_prob(sent, idx, alpha, vocab_size):\n",
    "    return math.log2((bigram_counter[(sent[idx-1], sent[idx])] + round(alpha,4)) / (unigram_counter[(sent[idx-1],)] + round(alpha,4)*vocab_size))\n",
    "\n",
    "def log_trigram_prob(sent, idx, alpha, vocab_size):\n",
    "    return math.log2((trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + alpha*vocab_size))\n",
    "\n",
    "def bigram(sents_tokenized):\n",
    "    bigram_cnt = 0\n",
    "    sum_prob = 0    \n",
    "    for sent in sents_tokenized:\n",
    "        sent = ['*start*'] + sent + ['*end*']\n",
    "        for idx in range(1,len(sent)):\n",
    "            sum_prob += log_bigram_prob(sent, idx, alpha, vocab_size)\n",
    "            bigram_cnt+=1\n",
    "    return sum_prob, bigram_cnt\n",
    "\n",
    "def trigram(sents_tokenized, alpha):\n",
    "    trigram_cnt = 0\n",
    "    sum_prob = 0    \n",
    "    for sent in sents_tokenized:\n",
    "        sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "        for idx in range(2,len(sent)):\n",
    "            sum_prob += log_trigram_prob(sent, idx, alpha, vocab_size)                                                   \n",
    "            trigram_cnt+=1\n",
    "    return sum_prob, trigram_cnt\n",
    "            \n",
    "def best_alpha(perpl,lowest_perplexity,lowest_alpha):\n",
    "    if(perpl<lowest_perplexity):\n",
    "        lowest_perplexity=perpl\n",
    "        lowest_alpha=round(alpha,4)\n",
    "    return lowest_perplexity, lowest_alpha\n",
    "\n",
    "def HC_entropy(sum_prob, ngram_cnt):\n",
    "    HC = -sum_prob / ngram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "    return HC, perpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning alpha wrt perplexity for the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha that produces the lowest perplexity for the bigram model is:  0.006\n"
     ]
    }
   ],
   "source": [
    "lowest_bigram_alpha=1\n",
    "lowest_perplexity=100000\n",
    "for alpha in np.arange(0.005,0.025,0.0005):\n",
    "    sum_prob, bigram_cnt = bigram(development_sents_tokenized)\n",
    "    HC, perpl = HC_entropy(sum_prob, bigram_cnt)\n",
    "    lowest_perplexity, lowest_bigram_alpha = best_alpha(perpl, lowest_perplexity, lowest_bigram_alpha)\n",
    "    \n",
    "print(\"The alpha that produces the lowest perplexity for the bigram model is: \", lowest_bigram_alpha)\n",
    "b_alpha=lowest_bigram_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning alpha wrt perplexity for the trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha that produces the lowest perplexity for the trigram model is:  0.0015\n"
     ]
    }
   ],
   "source": [
    "lowest_trigram_alpha=1\n",
    "lowest_perplexity=100000\n",
    "for alpha in np.arange(0.001,0.01,0.0005):\n",
    "    sum_prob, trigram_cnt = trigram(development_sents_tokenized, alpha)\n",
    "    HC, perpl = HC_entropy(sum_prob, bigram_cnt)\n",
    "    lowest_perplexity, lowest_trigram_alpha = best_alpha(perpl, lowest_perplexity, lowest_trigram_alpha)\n",
    "\n",
    "print(\"The alpha that produces the lowest perplexity for the trigram model is: \", lowest_trigram_alpha)\n",
    "t_alpha=lowest_trigram_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition for computing probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_sentences(sents_tokenized):\n",
    "    probabilities_list = []\n",
    "    sentences_list = []\n",
    "    for sent in sents_tokenized:\n",
    "        sum_prob = 0\n",
    "        sent = ['*start*'] + sent + ['*end*']\n",
    "        for idx in range(1,len(sent)):\n",
    "            sum_prob += log_bigram_prob(sent, idx, alpha, vocab_size)\n",
    "        probabilities_list.append(sum_prob)\n",
    "        sentences_list.append(' '.join(sent[1:len(sent)-2]))\n",
    "    return probabilities_list, sentences_list\n",
    "\n",
    "def trigram_sentences(sents_tokenized):\n",
    "    probabilities_list = []\n",
    "    sentences_list = []\n",
    "    for sent in sents_tokenized:\n",
    "        sum_prob = 0\n",
    "        sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "        for idx in range(2,len(sent)):\n",
    "            sum_prob += log_trigram_prob(sent, idx, alpha, vocab_size)\n",
    "        probabilities_list.append(sum_prob)\n",
    "        sentences_list.append(' '.join(sent[2:len(sent)-3]))\n",
    "    return probabilities_list, sentences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the Test set - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "      <th>Log-Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>In the agricultural sector , rapid restructuring is being called for in order to concentrate the land in a few hands and forge another link in the chain controlled by the network of multinationals</th>\n",
       "      <td>-266.395743</td>\n",
       "      <td>2.022083e-116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Furthermore , the liberalisation of trade and the abolition of duty and subsidies have hit agricultural production directly , reducing farmers ' incomes , *UNK* farming and increasing unemployment</th>\n",
       "      <td>-243.377202</td>\n",
       "      <td>2.007355e-106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agricultural production in Greece - and elsewhere - is being sacrificed in order to protect and corner a larger share of the international market for processed products from central and northern Europe</th>\n",
       "      <td>-257.540603</td>\n",
       "      <td>1.417546e-112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We believe that , rather than defending the interests of the people , the EU will again endeavour at the new round of talks to stake a bigger claim for the European monopolies , in competition with the other imperialist centres , i . e</th>\n",
       "      <td>-347.941678</td>\n",
       "      <td>7.777665e-152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the USA and Japan</th>\n",
       "      <td>-38.691652</td>\n",
       "      <td>1.571914e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Probability  \\\n",
       "Sentence                                                          \n",
       "In the agricultural sector , rapid restructurin...  -266.395743   \n",
       "Furthermore , the liberalisation of trade and t...  -243.377202   \n",
       "Agricultural production in Greece - and elsewhe...  -257.540603   \n",
       "We believe that , rather than defending the int...  -347.941678   \n",
       "the USA and Japan                                    -38.691652   \n",
       "\n",
       "                                                    Log-Probability  \n",
       "Sentence                                                             \n",
       "In the agricultural sector , rapid restructurin...    2.022083e-116  \n",
       "Furthermore , the liberalisation of trade and t...    2.007355e-106  \n",
       "Agricultural production in Greece - and elsewhe...    1.417546e-112  \n",
       "We believe that , rather than defending the int...    7.777665e-152  \n",
       "the USA and Japan                                      1.571914e-17  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_list, sentences_list = bigram_sentences(test_sents_tokenized)\n",
    "df_sent_bigram = pd.DataFrame({'Sentence': sentences_list, 'Probability': probabilities_list, 'Log-Probability': np.exp(probabilities_list)})\n",
    "df_sent_bigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences:\n",
      "Mean Probability: -197.0272754091232\n",
      "Mean Log Probability: 7.031446990887461e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences:\")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_list))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the Test set - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "      <th>Log-Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>In the agricultural sector , rapid restructuring is being called for in order to concentrate the land in a few hands and forge another link in the chain controlled by the network of multinationals</th>\n",
       "      <td>-332.989616</td>\n",
       "      <td>2.423524e-145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Furthermore , the liberalisation of trade and the abolition of duty and subsidies have hit agricultural production directly , reducing farmers ' incomes , *UNK* farming and increasing unemployment</th>\n",
       "      <td>-276.676814</td>\n",
       "      <td>6.930849e-121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agricultural production in Greece - and elsewhere - is being sacrificed in order to protect and corner a larger share of the international market for processed products from central and northern Europe</th>\n",
       "      <td>-329.409687</td>\n",
       "      <td>8.693415e-144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We believe that , rather than defending the interests of the people , the EU will again endeavour at the new round of talks to stake a bigger claim for the European monopolies , in competition with the other imperialist centres , i . e</th>\n",
       "      <td>-389.252624</td>\n",
       "      <td>8.907042e-170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the USA and Japan</th>\n",
       "      <td>-30.019313</td>\n",
       "      <td>9.178632e-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Probability  \\\n",
       "Sentence                                                          \n",
       "In the agricultural sector , rapid restructurin...  -332.989616   \n",
       "Furthermore , the liberalisation of trade and t...  -276.676814   \n",
       "Agricultural production in Greece - and elsewhe...  -329.409687   \n",
       "We believe that , rather than defending the int...  -389.252624   \n",
       "the USA and Japan                                    -30.019313   \n",
       "\n",
       "                                                    Log-Probability  \n",
       "Sentence                                                             \n",
       "In the agricultural sector , rapid restructurin...    2.423524e-145  \n",
       "Furthermore , the liberalisation of trade and t...    6.930849e-121  \n",
       "Agricultural production in Greece - and elsewhe...    8.693415e-144  \n",
       "We believe that , rather than defending the int...    8.907042e-170  \n",
       "the USA and Japan                                      9.178632e-14  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_list, sentences_list = trigram_sentences(test_sents_tokenized)\n",
    "df_sent_trigram = pd.DataFrame({'Sentence': sentences_list, 'Probability': probabilities_list, 'Log-Probability': np.exp(probabilities_list)})\n",
    "df_sent_trigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences:\n",
      "Mean Probability: -225.5061431192479\n",
      "Mean Log Probability: 0.016201686058953817\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences:\")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_list))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the random sentences - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_text(sents_tokenized):\n",
    "    text = []\n",
    "    for sent in sents_tokenized:\n",
    "        rand_sent = []\n",
    "        for i in range(len(sent)):\n",
    "            rand_sent.append(test_tokens[random.randint(0, len(test_tokens)-1)])\n",
    "        text.append(rand_sent)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "      <th>Log-Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>have awarded our extension Agency information for this of conferred Facility , , in are . wish whose out % Cooperation should wide huge work in October and Europe and to , confirmed obvious</th>\n",
       "      <td>-484.398147</td>\n",
       "      <td>4.251650e-211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achieved rather progress Union *UNK* Maij-Weggen remark , adopted regards State the ) public to been *UNK* action travelling Member can point years an why committee of believe paid</th>\n",
       "      <td>-463.632980</td>\n",
       "      <td>4.433585e-202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( which a animal *UNK* banks safety themselves We whether climate and , correctly Research the International before Socialist and democratic competition has to Access especially be , of Members . and</th>\n",
       "      <td>-424.079523</td>\n",
       "      <td>6.677336e-185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>They . % quite insist the its . five it *UNK* joint underpin reason and is accept . to , , are the active up on which I also I Europe clearly is , , your which those and true security an this employment conversion</th>\n",
       "      <td>-660.718836</td>\n",
       "      <td>1.130982e-287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunnel position then of</th>\n",
       "      <td>-101.373244</td>\n",
       "      <td>9.422353e-45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Probability  \\\n",
       "Sentence                                                          \n",
       "have awarded our extension Agency information f...  -484.398147   \n",
       "achieved rather progress Union *UNK* Maij-Wegge...  -463.632980   \n",
       "( which a animal *UNK* banks safety themselves ...  -424.079523   \n",
       "They . % quite insist the its . five it *UNK* j...  -660.718836   \n",
       "tunnel position then of                             -101.373244   \n",
       "\n",
       "                                                    Log-Probability  \n",
       "Sentence                                                             \n",
       "have awarded our extension Agency information f...    4.251650e-211  \n",
       "achieved rather progress Union *UNK* Maij-Wegge...    4.433585e-202  \n",
       "( which a animal *UNK* banks safety themselves ...    6.677336e-185  \n",
       "They . % quite insist the its . five it *UNK* j...    1.130982e-287  \n",
       "tunnel position then of                                9.422353e-45  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_random, sentences_random = bigram_sentences(create_random_text(test_sents_tokenized))\n",
    "df_sent_rand_bigram = pd.DataFrame({'Sentence': sentences_random, 'Probability': probabilities_random, 'Log-Probability': np.exp(probabilities_random)})\n",
    "df_sent_rand_bigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences: \n",
      "Mean Probability: -409.31964839900786\n",
      "Mean Log Probability: 1.7878202905338187e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences: \")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_random))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the random sentences - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "      <th>Log-Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>are to legislation official and hope something . contacts compensate I main account in . course , . should have Conference on Member resumption legislation to especially John second environmental of municipal to a</th>\n",
       "      <td>-508.550584</td>\n",
       "      <td>1.378121e-221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an of to justice 1 be participate we this trigger the EU this to all four again even the it I , the phenomenon we and , , present</th>\n",
       "      <td>-425.989360</td>\n",
       "      <td>9.889440e-186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No development problem full we glad interventions the only thoughts area the statement far *UNK* Greek few likewise over random , once intends this the first employment what consider concerning *UNK* participation</th>\n",
       "      <td>-430.771105</td>\n",
       "      <td>8.288698e-188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>must which his I - its Mr . an agree the of who Convention the question and will following the in return priorities We . also enterprise for 80 the range set other the on Convention as , define of by the Secondly *UNK* politics</th>\n",
       "      <td>-629.517616</td>\n",
       "      <td>4.017659e-274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on . in retain</th>\n",
       "      <td>-84.100018</td>\n",
       "      <td>2.991069e-37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Probability  \\\n",
       "Sentence                                                          \n",
       "are to legislation official and hope something ...  -508.550584   \n",
       "an of to justice 1 be participate we this trigg...  -425.989360   \n",
       "No development problem full we glad interventio...  -430.771105   \n",
       "must which his I - its Mr . an agree the of who...  -629.517616   \n",
       "on . in retain                                       -84.100018   \n",
       "\n",
       "                                                    Log-Probability  \n",
       "Sentence                                                             \n",
       "are to legislation official and hope something ...    1.378121e-221  \n",
       "an of to justice 1 be participate we this trigg...    9.889440e-186  \n",
       "No development problem full we glad interventio...    8.288698e-188  \n",
       "must which his I - its Mr . an agree the of who...    4.017659e-274  \n",
       "on . in retain                                         2.991069e-37  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_random, sentences_random = trigram_sentences(create_random_text(test_sents_tokenized))\n",
    "df_sent_rand_trigram = pd.DataFrame({'Sentence': sentences_random, 'Probability': probabilities_random, 'Log-Probability': np.exp(probabilities_random)})\n",
    "df_sent_rand_trigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences: \n",
      "Mean Probability: -396.51164876638256\n",
      "Mean Log Probability: 0.0005975755979599098\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences: \")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_random))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can clearly see a difference in the probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of the whole corpus as a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding only *end* (without *start*) to all test sentences - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_sent=[]\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sent = sent + ['*end*']\n",
    "    test_one_sent.extend(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating HC and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 6.954\n",
      "Perplexity: 123.977\n"
     ]
    }
   ],
   "source": [
    "bigram_cnt = 0\n",
    "sum_prob = 0\n",
    "for idx in range(1,len(test_one_sent)):\n",
    "    bigram_prob = (bigram_counter[(test_one_sent[idx-1], test_one_sent[idx])] + b_alpha) / (unigram_counter[(test_one_sent[idx-1],)] + b_alpha*vocab_size)\n",
    "    sum_prob += math.log2(bigram_prob)\n",
    "    bigram_cnt+=1\n",
    "\n",
    "HC = -sum_prob / bigram_cnt\n",
    "perpl = math.pow(2,HC)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"Perplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding only *end* (without *start*) to all test sentences - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_sent=[]\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sent = sent + ['*end*'] + ['*end*']\n",
    "    test_one_sent.extend(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating HC and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 8.118\n",
      "Perplexity: 277.859\n"
     ]
    }
   ],
   "source": [
    "trigram_cnt = 0\n",
    "sum_prob = 0\n",
    "for idx in range(2,len(test_one_sent)):\n",
    "    trigram_prob = (trigram_counter[(test_one_sent[idx-2],test_one_sent[idx-1], test_one_sent[idx])] +t_alpha) / (bigram_counter[(test_one_sent[idx-2],test_one_sent[idx-1])] + t_alpha*vocab_size)\n",
    "    sum_prob += math.log2(trigram_prob)\n",
    "    trigram_cnt+=1\n",
    "\n",
    "HC = -sum_prob / trigram_cnt\n",
    "perpl = math.pow(2,HC)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"Perplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning for ideal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>121.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>122.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>124.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>125.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>126.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>127.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>128.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>129.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>130.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45</th>\n",
       "      <td>132.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>133.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>134.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>135.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.65</th>\n",
       "      <td>136.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>138.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>139.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>140.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.85</th>\n",
       "      <td>141.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>143.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>144.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>145.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Perplexity\n",
       "lambda            \n",
       "0.00        121.80\n",
       "0.05        122.89\n",
       "0.10        124.00\n",
       "0.15        125.11\n",
       "0.20        126.23\n",
       "0.25        127.36\n",
       "0.30        128.51\n",
       "0.35        129.66\n",
       "0.40        130.82\n",
       "0.45        132.00\n",
       "0.50        133.18\n",
       "0.55        134.38\n",
       "0.60        135.58\n",
       "0.65        136.80\n",
       "0.70        138.03\n",
       "0.75        139.27\n",
       "0.80        140.52\n",
       "0.85        141.78\n",
       "0.90        143.05\n",
       "0.95        144.33\n",
       "1.00        145.63"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'lambda': [], 'Perplexity': []})\n",
    "lowest_lamda = 2\n",
    "lowest_perplexity = 100000\n",
    "for lamda in np.arange(0,1.01,0.05):\n",
    "    ngram_cnt = 0\n",
    "    sum_prob = 0\n",
    "    for sent in development_sents_tokenized:\n",
    "        sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "        for idx in range(2,len(sent)):\n",
    "            bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +b_alpha) / (unigram_counter[(sent[idx-1],)] + b_alpha*vocab_size)\n",
    "            trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + t_alpha*vocab_size)\n",
    "            sum_prob += ((1-lamda) * math.log2(bigram_prob)) + (lamda * math.log2(trigram_prob))\n",
    "            ngram_cnt+=1\n",
    "\n",
    "    HC, perpl = HC_entropy(sum_prob, ngram_cnt)\n",
    "    if(perpl<lowest_perplexity):\n",
    "        lowest_perplexity=perpl\n",
    "        lowest_lamda=lamda\n",
    "    df = df.append({'lambda': round(lamda,2),  'Perplexity': round(perpl,2)}, ignore_index=True)\n",
    "df.set_index('lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lambda that produces the lowest perplexity for the interpolated model is:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The lambda that produces the lowest perplexity for the interpolated model is: \", lowest_lamda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
