{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize,TweetTokenizer #Tokenizing sentences\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = open(\"europarl-v7-en.txt\",encoding = \"utf8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split source into training / development / test set - 70/10/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sents = sent_tokenize(source)\n",
    "sents_len=len(source_sents)\n",
    "#source_sents=source_sents[0:sents_len]\n",
    "train_sents=source_sents[0:round(sents_len*0.7)]\n",
    "development_sents=source_sents[round(sents_len*0.7):round(sents_len*0.8)]\n",
    "test_sents=source_sents[round(sents_len*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sets by Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_wt = TweetTokenizer()\n",
    "\n",
    "train_tokens = tweet_wt.tokenize(' '.join(train_sents))\n",
    "development_tokens = tweet_wt.tokenize(' '.join(development_sents))\n",
    "test_tokens = tweet_wt.tokenize(' '.join(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding tokens appearing in the training test rarely... Replacing those with *UKN* in all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(train_tokens)\n",
    "frequent_tokens=set([k for k,v in count.items() if v >= 10])\n",
    "train_tokens=[train_token if (train_token in frequent_tokens) else \"*UNK*\" for train_token in train_tokens]\n",
    "development_tokens=[development_token if (development_token in frequent_tokens) else \"*UNK*\" for development_token in development_tokens]\n",
    "test_tokens=[test_token if (test_token in frequent_tokens) else \"*UNK*\" for test_token in test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sets by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_tokenized = []\n",
    "development_sents_tokenized = []\n",
    "test_sents_tokenized = []\n",
    "\n",
    "for sent in train_sents:\n",
    "    sent_tmp = tweet_wt.tokenize(sent)\n",
    "    sent = [word if (word in frequent_tokens) else \"*UNK*\" for word in sent_tmp]\n",
    "    train_sents_tokenized.append(sent)\n",
    "for sent in development_sents:\n",
    "    sent_tmp = tweet_wt.tokenize(sent)\n",
    "    sent = [word if (word in frequent_tokens) else \"*UNK*\" for word in sent_tmp]\n",
    "    development_sents_tokenized.append(sent)\n",
    "for sent in test_sents:\n",
    "    sent_tmp = tweet_wt.tokenize(sent)\n",
    "    sent = [word if (word in frequent_tokens) else \"*UNK*\" for word in sent_tmp]\n",
    "    test_sents_tokenized.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tokens into Bigram and Trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()\n",
    "\n",
    "for sent in train_sents_tokenized:\n",
    "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    \n",
    "unigram_counter[('*start*',)] = len(train_sents_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning alpha wrt perplexity for the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha that produces the lowest perplexity for the bigram model is:  0.006\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(set(train_tokens))\n",
    "lowest_brigram_alpha=1\n",
    "lowest_perplexity=100000\n",
    "for alpha in np.arange(0.005,0.025,0.0005):\n",
    "    #Reseting probability for each alpha\n",
    "    bigram_cnt = 0\n",
    "    sum_prob = 0\n",
    "    for sent in development_sents_tokenized:\n",
    "        sent = ['*start*'] + sent + ['*end*']\n",
    "        for idx in range(1,len(sent)):\n",
    "            bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] + round(alpha,4)) / (unigram_counter[(sent[idx-1],)] + round(alpha,4)*vocab_size)\n",
    "            sum_prob += math.log2(bigram_prob)\n",
    "            bigram_cnt+=1\n",
    "\n",
    "    HC = -sum_prob / bigram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "    #print(\"For alpha=\",round(alpha,4),\"perplexity: {0:.3f}\".format(perpl))\n",
    "    if(perpl<lowest_perplexity):\n",
    "        lowest_perplexity=perpl\n",
    "        lowest_brigram_alpha=round(alpha,4)\n",
    "print(\"The alpha that produces the lowest perplexity for the bigram model is: \", lowest_brigram_alpha)\n",
    "b_alpha=lowest_brigram_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning alpha wrt perplexity for the trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha that produces the lowest perplexity for the trigram model is:  0.0015\n"
     ]
    }
   ],
   "source": [
    "lowest_trigram_alpha=1\n",
    "lowest_perplexity=100000\n",
    "for alpha in np.arange(0.001,0.01,0.0005):\n",
    "    trigram_cnt = 0\n",
    "    sum_prob = 0    \n",
    "    for sent in development_sents_tokenized:\n",
    "        sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "        for idx in range(2,len(sent)):\n",
    "            trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + alpha*vocab_size)\n",
    "            sum_prob += math.log2(trigram_prob)\n",
    "            trigram_cnt+=1\n",
    "\n",
    "    HC = -sum_prob / trigram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "    #print(\"For alpha=\",round(alpha,4),\"perplexity: {0:.3f}\".format(perpl))\n",
    "    if(perpl<lowest_perplexity):\n",
    "        lowest_perplexity=perpl\n",
    "        lowest_trigram_alpha=round(alpha,4)\n",
    "print(\"The alpha that produces the lowest perplexity for the trigram model is: \", lowest_trigram_alpha)\n",
    "t_alpha=lowest_trigram_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the Test set - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log-Probability</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>In the agricultural sector , rapid restructuring is being called for in order to concentrate the land in a few hands and forge another link in the chain controlled by the network of multinationals .</th>\n",
       "      <td>-265.154277</td>\n",
       "      <td>6.997787e-116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Furthermore , the liberalisation of trade and the abolition of duty and subsidies have hit agricultural production directly , reducing farmers ' incomes , *UNK* farming and increasing unemployment .</th>\n",
       "      <td>-242.691735</td>\n",
       "      <td>3.983994e-106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agricultural production in Greece - and elsewhere - is being sacrificed in order to protect and corner a larger share of the international market for processed products from central and northern Europe .</th>\n",
       "      <td>-256.218822</td>\n",
       "      <td>5.315933e-112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We believe that , rather than defending the interests of the people , the EU will again endeavour at the new round of talks to stake a bigger claim for the European monopolies , in competition with the other imperialist centres , i . e .</th>\n",
       "      <td>-349.601468</td>\n",
       "      <td>1.479147e-152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the USA and Japan .</th>\n",
       "      <td>-38.140553</td>\n",
       "      <td>2.727522e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                               Log-Probability  \\\n",
       "Sentence                                                                                                                                                                                                                                                         \n",
       "In the agricultural sector , rapid restructuring is being called for in order to concentrate the land in a few hands and forge another link in the chain controlled by the network of multinationals .                                        -265.154277        \n",
       "Furthermore , the liberalisation of trade and the abolition of duty and subsidies have hit agricultural production directly , reducing farmers ' incomes , *UNK* farming and increasing unemployment .                                        -242.691735        \n",
       "Agricultural production in Greece - and elsewhere - is being sacrificed in order to protect and corner a larger share of the international market for processed products from central and northern Europe .                                   -256.218822        \n",
       "We believe that , rather than defending the interests of the people , the EU will again endeavour at the new round of talks to stake a bigger claim for the European monopolies , in competition with the other imperialist centres , i . e . -349.601468        \n",
       "the USA and Japan .                                                                                                                                                                                                                           -38.140553         \n",
       "\n",
       "                                                                                                                                                                                                                                                 Probability  \n",
       "Sentence                                                                                                                                                                                                                                                      \n",
       "In the agricultural sector , rapid restructuring is being called for in order to concentrate the land in a few hands and forge another link in the chain controlled by the network of multinationals .                                         6.997787e-116  \n",
       "Furthermore , the liberalisation of trade and the abolition of duty and subsidies have hit agricultural production directly , reducing farmers ' incomes , *UNK* farming and increasing unemployment .                                         3.983994e-106  \n",
       "Agricultural production in Greece - and elsewhere - is being sacrificed in order to protect and corner a larger share of the international market for processed products from central and northern Europe .                                    5.315933e-112  \n",
       "We believe that , rather than defending the interests of the people , the EU will again endeavour at the new round of talks to stake a bigger claim for the European monopolies , in competition with the other imperialist centres , i . e .  1.479147e-152  \n",
       "the USA and Japan .                                                                                                                                                                                                                            2.727522e-17   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_count=0\n",
    "probabilities_test=[]\n",
    "df_sent_bigram = pd.DataFrame({'Sentence': [], 'Log-Probability': [], 'Probability':[]})\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sum_prob = 0\n",
    "    sent = ['*start*'] + sent + ['*end*']\n",
    "    for idx in range(1,len(sent)):\n",
    "        bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +b_alpha) / (unigram_counter[(sent[idx-1],)] + b_alpha*vocab_size)\n",
    "        sum_prob += math.log2(bigram_prob)\n",
    "        \n",
    "        sent_count+=1\n",
    "    probabilities_test.append(sum_prob)\n",
    "    df_sent_bigram = df_sent_bigram.append({'Sentence': ' '.join(sent[1:len(sent)-1]),  \"Log-Probability\":sum_prob, \"Probability\": np.exp(sum_prob)},ignore_index=True)\n",
    "    #print(\"Sentence: \",' '.join(sent[1:len(sent)-1]),\"\\nLog Probability: \",sum_prob, \"\\nProbability: \",np.exp(sum_prob),\"\\n\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)  \n",
    "df_sent_bigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences:\n",
      "Mean Probability: -196.81242769991334\n",
      "Mean Log Probability: 7.039408974668455e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences:\")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_test))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_test)))\n",
    "# df_mean_bigram = pd.DataFrame({'Mean Probability': [], 'Mean Log Probability': []})\n",
    "# df_mean_bigram = df_mean_bigram.append({'Mean Probability': np.mean(probabilities_test), 'Mean Log Probability': np.mean(np.exp(probabilities_test))},ignore_index=True)\n",
    "\n",
    "# df_mean_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the Test set - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log-Probability</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>In the agricultural sector , rapid restructuring is being called for in order to concentrate the land in a few hands and forge another link in the chain controlled by the network of multinationals .</th>\n",
       "      <td>-326.922753</td>\n",
       "      <td>1.045327e-142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Furthermore , the liberalisation of trade and the abolition of duty and subsidies have hit agricultural production directly , reducing farmers ' incomes , *UNK* farming and increasing unemployment .</th>\n",
       "      <td>-268.100934</td>\n",
       "      <td>3.674886e-117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agricultural production in Greece - and elsewhere - is being sacrificed in order to protect and corner a larger share of the international market for processed products from central and northern Europe .</th>\n",
       "      <td>-321.887925</td>\n",
       "      <td>1.606388e-140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We believe that , rather than defending the interests of the people , the EU will again endeavour at the new round of talks to stake a bigger claim for the European monopolies , in competition with the other imperialist centres , i . e .</th>\n",
       "      <td>-385.237366</td>\n",
       "      <td>4.937849e-168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the USA and Japan .</th>\n",
       "      <td>-23.819847</td>\n",
       "      <td>4.520348e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                               Log-Probability  \\\n",
       "Sentence                                                                                                                                                                                                                                                         \n",
       "In the agricultural sector , rapid restructuring is being called for in order to concentrate the land in a few hands and forge another link in the chain controlled by the network of multinationals .                                        -326.922753        \n",
       "Furthermore , the liberalisation of trade and the abolition of duty and subsidies have hit agricultural production directly , reducing farmers ' incomes , *UNK* farming and increasing unemployment .                                        -268.100934        \n",
       "Agricultural production in Greece - and elsewhere - is being sacrificed in order to protect and corner a larger share of the international market for processed products from central and northern Europe .                                   -321.887925        \n",
       "We believe that , rather than defending the interests of the people , the EU will again endeavour at the new round of talks to stake a bigger claim for the European monopolies , in competition with the other imperialist centres , i . e . -385.237366        \n",
       "the USA and Japan .                                                                                                                                                                                                                           -23.819847         \n",
       "\n",
       "                                                                                                                                                                                                                                                 Probability  \n",
       "Sentence                                                                                                                                                                                                                                                      \n",
       "In the agricultural sector , rapid restructuring is being called for in order to concentrate the land in a few hands and forge another link in the chain controlled by the network of multinationals .                                         1.045327e-142  \n",
       "Furthermore , the liberalisation of trade and the abolition of duty and subsidies have hit agricultural production directly , reducing farmers ' incomes , *UNK* farming and increasing unemployment .                                         3.674886e-117  \n",
       "Agricultural production in Greece - and elsewhere - is being sacrificed in order to protect and corner a larger share of the international market for processed products from central and northern Europe .                                    1.606388e-140  \n",
       "We believe that , rather than defending the interests of the people , the EU will again endeavour at the new round of talks to stake a bigger claim for the European monopolies , in competition with the other imperialist centres , i . e .  4.937849e-168  \n",
       "the USA and Japan .                                                                                                                                                                                                                            4.520348e-11   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_count=0\n",
    "probabilities_test=[]\n",
    "df_sent_trigram = pd.DataFrame({'Sentence': [], 'Log-Probability': [], 'Probability':[]})\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sum_prob = 0\n",
    "    sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "    for idx in range(2,len(sent)):\n",
    "        trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + t_alpha*vocab_size)\n",
    "        #trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-1],sent[idx])] + t_alpha*vocab_size)\n",
    "        sum_prob += math.log2(trigram_prob)\n",
    "        \n",
    "        sent_count+=1\n",
    "    probabilities_test.append(sum_prob)\n",
    "    df_sent_trigram = df_sent_trigram.append({'Sentence': ' '.join(sent[2:len(sent)-2]),  \"Log-Probability\":sum_prob, \"Probability\": np.exp(sum_prob)},ignore_index=True)\n",
    "\n",
    "df_sent_trigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences: \n",
      "Mean Probability: -218.50332321483137\n",
      "Mean Log Probability: 0.3728236029654936\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences: \")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_test))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the random sentences - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log-Probability</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>to this useful of exactly from have as composition the *UNK* *UNK* is *UNK* the peace contributed fruit world , which is I under been four visits , risks , ' Affairs in energy solved</th>\n",
       "      <td>-436.002537</td>\n",
       "      <td>4.431024e-190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>here matter of such next to where is realise the destroyed measures full us At item , *UNK* of of , unanimous another on may m is then debate but</th>\n",
       "      <td>-429.660071</td>\n",
       "      <td>2.517690e-187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the the 30 in fully the to send Mr adjourned should them going *UNK* that fit is - , to like essential the , animal adoption hour We the was for , to</th>\n",
       "      <td>-499.620301</td>\n",
       "      <td>1.041503e-217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use plenary me m policies package after , of is to see that , which September proposals *UNK* discussed not of of want shows provides will I its of coordinators to peace and , the efficient as define the If He and were the . are</th>\n",
       "      <td>-640.711701</td>\n",
       "      <td>5.526418e-279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starting employment without an as</th>\n",
       "      <td>-89.458562</td>\n",
       "      <td>1.408122e-39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                      Log-Probability  \\\n",
       "Sentence                                                                                                                                                                                                                                                \n",
       "to this useful of exactly from have as composition the *UNK* *UNK* is *UNK* the peace contributed fruit world , which is I under been four visits , risks , ' Affairs in energy solved                                               -436.002537        \n",
       "here matter of such next to where is realise the destroyed measures full us At item , *UNK* of of , unanimous another on may m is then debate but                                                                                    -429.660071        \n",
       "the the 30 in fully the to send Mr adjourned should them going *UNK* that fit is - , to like essential the , animal adoption hour We the was for , to                                                                                -499.620301        \n",
       "use plenary me m policies package after , of is to see that , which September proposals *UNK* discussed not of of want shows provides will I its of coordinators to peace and , the efficient as define the If He and were the . are -640.711701        \n",
       "starting employment without an as                                                                                                                                                                                                    -89.458562         \n",
       "\n",
       "                                                                                                                                                                                                                                        Probability  \n",
       "Sentence                                                                                                                                                                                                                                             \n",
       "to this useful of exactly from have as composition the *UNK* *UNK* is *UNK* the peace contributed fruit world , which is I under been four visits , risks , ' Affairs in energy solved                                                4.431024e-190  \n",
       "here matter of such next to where is realise the destroyed measures full us At item , *UNK* of of , unanimous another on may m is then debate but                                                                                     2.517690e-187  \n",
       "the the 30 in fully the to send Mr adjourned should them going *UNK* that fit is - , to like essential the , animal adoption hour We the was for , to                                                                                 1.041503e-217  \n",
       "use plenary me m policies package after , of is to see that , which September proposals *UNK* discussed not of of want shows provides will I its of coordinators to peace and , the efficient as define the If He and were the . are  5.526418e-279  \n",
       "starting employment without an as                                                                                                                                                                                                     1.408122e-39   "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the length of sentences in the test set to create new sentences of approximately equal size\n",
    "probabilities_random=[]\n",
    "df_sent_rand_bigram = pd.DataFrame({'Sentence': [], 'Log-Probability': [], 'Probability':[]})\n",
    "#I will create the same number of sentences, the test set contains (33444)\n",
    "for i in range(0,len(test_sents)):\n",
    "    sent=[]\n",
    "    for j in range(len(test_sents_tokenized[i])):\n",
    "        #Each time I pick a random word from the test set\n",
    "        sent.append(test_tokens[random.randint(0, len(test_tokens)-1)])\n",
    "    sum_prob = 0\n",
    "    sent = ['*start*'] + sent + ['*end*']\n",
    "    for idx in range(1,len(sent)):\n",
    "        bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +b_alpha) / (unigram_counter[(sent[idx-1],)] + b_alpha*vocab_size)\n",
    "        sum_prob += math.log2(bigram_prob)\n",
    "    probabilities_random.append(sum_prob)\n",
    "    df_sent_rand_bigram = df_sent_rand_bigram.append({'Sentence': ' '.join(sent[1:len(sent)-1]),  \"Log-Probability\":sum_prob, \"Probability\": np.exp(sum_prob)},ignore_index=True)\n",
    "\n",
    "df_sent_rand_bigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences: \n",
      "Mean Probability: -417.6092893327352\n",
      "Mean Log Probability: 1.1929618387029627e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences: \")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_random))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the random sentences - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log-Probability</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>delegation . very economic necessarily , the in institutions , matter violence , the *UNK* , bananas is of *UNK* port that to this . to ' is development available , . up , flags</th>\n",
       "      <td>-491.248619</td>\n",
       "      <td>4.502310e-214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( the since aim on must is accepted . responsibility equality paragraphs a that UN can involved When parliamentary a financial further still close That Mr five This to dates</th>\n",
       "      <td>-410.380474</td>\n",
       "      <td>5.943259e-179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>' was regard the others obligations reiterating like all Diamantopoulou extend transport at during however in . all to States moment countries close and . is parliaments de the chosen four where them</th>\n",
       "      <td>-463.842826</td>\n",
       "      <td>3.594349e-202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>. amendments the extended accidents has that various of months consequences our concentration Madam inadequate offices in State the to , food 17 Monetary also , : manufactured first scope sustainable deep that Spain should make this the satisfactory will gap ' , it the be</th>\n",
       "      <td>-638.299577</td>\n",
       "      <td>6.166174e-278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about have ) stem not</th>\n",
       "      <td>-93.130090</td>\n",
       "      <td>3.581920e-41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                  Log-Probability  \\\n",
       "Sentence                                                                                                                                                                                                                                                                                            \n",
       "delegation . very economic necessarily , the in institutions , matter violence , the *UNK* , bananas is of *UNK* port that to this . to ' is development available , . up , flags                                                                                                -491.248619        \n",
       "( the since aim on must is accepted . responsibility equality paragraphs a that UN can involved When parliamentary a financial further still close That Mr five This to dates                                                                                                    -410.380474        \n",
       "' was regard the others obligations reiterating like all Diamantopoulou extend transport at during however in . all to States moment countries close and . is parliaments de the chosen four where them                                                                          -463.842826        \n",
       ". amendments the extended accidents has that various of months consequences our concentration Madam inadequate offices in State the to , food 17 Monetary also , : manufactured first scope sustainable deep that Spain should make this the satisfactory will gap ' , it the be -638.299577        \n",
       "about have ) stem not                                                                                                                                                                                                                                                            -93.130090         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    Probability  \n",
       "Sentence                                                                                                                                                                                                                                                                                         \n",
       "delegation . very economic necessarily , the in institutions , matter violence , the *UNK* , bananas is of *UNK* port that to this . to ' is development available , . up , flags                                                                                                 4.502310e-214  \n",
       "( the since aim on must is accepted . responsibility equality paragraphs a that UN can involved When parliamentary a financial further still close That Mr five This to dates                                                                                                     5.943259e-179  \n",
       "' was regard the others obligations reiterating like all Diamantopoulou extend transport at during however in . all to States moment countries close and . is parliaments de the chosen four where them                                                                           3.594349e-202  \n",
       ". amendments the extended accidents has that various of months consequences our concentration Madam inadequate offices in State the to , food 17 Monetary also , : manufactured first scope sustainable deep that Spain should make this the satisfactory will gap ' , it the be  6.166174e-278  \n",
       "about have ) stem not                                                                                                                                                                                                                                                             3.581920e-41   "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the length of sentences in the test set to create new sentences of approximately equal size\n",
    "probabilities_random=[]\n",
    "df_sent_rand_trigram = pd.DataFrame({'Sentence': [], 'Log-Probability': [], 'Probability':[]})\n",
    "#I will create the same number of sentences, the test set contains (33444)\n",
    "for i in range(0,len(test_sents)):\n",
    "    sent=[]\n",
    "    for j in range(len(test_sents_tokenized[i])):\n",
    "        #Each time I pick a random word from the test set\n",
    "        sent.append(test_tokens[random.randint(0, len(test_tokens)-1)])\n",
    "    sum_prob = 0\n",
    "    sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "    for idx in range(2,len(sent)):\n",
    "        trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + t_alpha*vocab_size)\n",
    "        #trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-1],sent[idx])] + t_alpha*vocab_size)\n",
    "        sum_prob += math.log2(trigram_prob)\n",
    "    probabilities_random.append(sum_prob)\n",
    "    df_sent_rand_trigram = df_sent_rand_trigram.append({'Sentence': ' '.join(sent[2:len(sent)-2]),  \"Log-Probability\":sum_prob, \"Probability\": np.exp(sum_prob)},ignore_index=True)\n",
    "\n",
    "        \n",
    "df_sent_rand_trigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences: \n",
      "Mean Probability: -406.7445951225196\n",
      "Mean Log Probability: 0.011642462651415454\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences: \")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_random))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can clearly see a huge difference in the probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of the whole corpus as a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding only \\*end\\* (without \\*start\\*) to all test sentences - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_sent=[]\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sent = sent + ['*end*']\n",
    "    test_one_sent.extend(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating HC and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 6.954\n",
      "Perplexity: 123.977\n"
     ]
    }
   ],
   "source": [
    "bigram_cnt = 0\n",
    "sum_prob = 0\n",
    "for idx in range(1,len(test_one_sent)):\n",
    "    bigram_prob = (bigram_counter[(test_one_sent[idx-1], test_one_sent[idx])] + b_alpha) / (unigram_counter[(test_one_sent[idx-1],)] + b_alpha*vocab_size)\n",
    "    sum_prob += math.log2(bigram_prob)\n",
    "    bigram_cnt+=1\n",
    "\n",
    "HC = -sum_prob / bigram_cnt\n",
    "perpl = math.pow(2,HC)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"Perplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding only \\*end\\* (without \\*start\\*) to all test sentences - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_sent=[]\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sent = sent + ['*end*'] + ['*end*']\n",
    "    test_one_sent.extend(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating HC and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 8.118\n",
      "Perplexity: 277.859\n"
     ]
    }
   ],
   "source": [
    "trigram_cnt = 0\n",
    "sum_prob = 0\n",
    "for idx in range(2,len(test_one_sent)):\n",
    "    trigram_prob = (trigram_counter[(test_one_sent[idx-2],test_one_sent[idx-1], test_one_sent[idx])] +t_alpha) / (bigram_counter[(test_one_sent[idx-2],test_one_sent[idx-1])] + t_alpha*vocab_size)\n",
    "    #trigram_prob = (trigram_counter[(test_one_sent[idx-2],test_one_sent[idx-1], test_one_sent[idx])] +t_alpha) / (bigram_counter[(test_one_sent[idx-1],test_one_sent[idx])] + t_alpha*vocab_size)\n",
    "    sum_prob += math.log2(trigram_prob)\n",
    "    trigram_cnt+=1\n",
    "\n",
    "HC = -sum_prob / trigram_cnt\n",
    "perpl = math.pow(2,HC)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"Perplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning for ideal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'lambda': [], 'Perplexity': []})\n",
    "lowest_lamda = 2\n",
    "lowest_perplexity = 100000\n",
    "for lamda in np.arange(0,1.01,0.05):\n",
    "    ngram_cnt = 0\n",
    "    sum_prob = 0\n",
    "    for sent in development_sents_tokenized:\n",
    "        sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "        for idx in range(2,len(sent)):\n",
    "            trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + t_alpha*vocab_size)\n",
    "            #trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-1],sent[idx])] + t_alpha*vocab_size)\n",
    "            bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +b_alpha) / (unigram_counter[(sent[idx-1],)] + b_alpha*vocab_size)\n",
    "\n",
    "            sum_prob += (lamda * math.log2(trigram_prob)) +((1-lamda) * math.log2(bigram_prob))\n",
    "            ngram_cnt+=1 \n",
    "\n",
    "    HC = -sum_prob / ngram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "    if(perpl<lowest_perplexity):\n",
    "        lowest_perplexity=perpl\n",
    "        lowest_lamda=lamda\n",
    "    df = df.append({'lambda': round(lamda,2),  'Perplexity': round(perpl,2)}, ignore_index=True)\n",
    "df.set_index('lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The lambda that produces the lowest perplexity for the interpolated model is: \", lowest_lamda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
