{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize,TweetTokenizer #Tokenizing sentences\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = open(\"europarl-v7-en-full.txt\",encoding = \"utf8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split source into training / development / test set - 70/10/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sents = sent_tokenize(source)\n",
    "sents_len=len(source_sents)\n",
    "source_sents=source_sents[0:sents_len]\n",
    "train_sents=source_sents[0:round(sents_len*0.7)]\n",
    "development_sents=source_sents[round(sents_len*0.7):round(sents_len*0.8)]\n",
    "test_sents=source_sents[round(sents_len*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sets by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_wt = TweetTokenizer()\n",
    "\n",
    "train_sents_tokenized = []\n",
    "development_sents_tokenized = []\n",
    "test_sents_tokenized = []\n",
    "for sent in train_sents:\n",
    "    train_sents_tokenized.append(tweet_wt.tokenize(sent))\n",
    "for sent in development_sents:\n",
    "    development_sents_tokenized.append(tweet_wt.tokenize(sent))\n",
    "for sent in test_sents:\n",
    "    test_sents_tokenized.append(tweet_wt.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sets by Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tweet_wt.tokenize(' '.join(train_sents))\n",
    "development_tokens = tweet_wt.tokenize(' '.join(development_sents))\n",
    "test_tokens = tweet_wt.tokenize(' '.join(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding tokens appearing in the training test rarely... Replacing those with *UKN* in all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(train_tokens)\n",
    "frequent_tokens=set([k for k,v in count.items() if v >= 10])\n",
    "train_tokens=[train_token if (train_token in frequent_tokens) else \"*UNK*\" for train_token in train_tokens]\n",
    "development_tokens=[development_token if (development_token in frequent_tokens) else \"*UNK*\" for development_token in development_tokens]\n",
    "test_tokens=[test_token if (test_token in frequent_tokens) else \"*UNK*\" for test_token in test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tokens into Bigram and Trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()\n",
    "\n",
    "for sent in train_sents_tokenized:\n",
    "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    \n",
    "unigram_counter[('*start*',)] = len(train_sents_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning alpha wrt perplexity for the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha that produces the lowest perplexity for the bigram model is:  0.005\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(set(train_tokens))\n",
    "lowest_brigram_alpha=1\n",
    "lowest_perplexity=100000\n",
    "for alpha in np.arange(0.005,0.025,0.0005):\n",
    "    #Reseting probability for each alpha\n",
    "    bigram_cnt = 0\n",
    "    sum_prob = 0\n",
    "    for sent in development_sents_tokenized:\n",
    "        sent = ['*start*'] + sent + ['*end*']\n",
    "        for idx in range(1,len(sent)):\n",
    "            bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] + round(alpha,4)) / (unigram_counter[(sent[idx-1],)] + round(alpha,4)*vocab_size)\n",
    "            sum_prob += math.log2(bigram_prob)\n",
    "            bigram_cnt+=1\n",
    "\n",
    "    HC = -sum_prob / bigram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "    #print(\"For alpha=\",round(alpha,4),\"perplexity: {0:.3f}\".format(perpl))\n",
    "    if(perpl<lowest_perplexity):\n",
    "        lowest_perplexity=perpl\n",
    "        lowest_brigram_alpha=round(alpha,4)\n",
    "print(\"The alpha that produces the lowest perplexity for the bigram model is: \", lowest_brigram_alpha)\n",
    "b_alpha=lowest_brigram_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning alpha wrt perplexity for the trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha that produces the lowest perplexity for the trigram model is:  0.001\n"
     ]
    }
   ],
   "source": [
    "lowest_trigram_alpha=1\n",
    "lowest_perplexity=100000\n",
    "for alpha in np.arange(0.001,0.01,0.0005):\n",
    "    trigram_cnt = 0\n",
    "    sum_prob = 0    \n",
    "    for sent in development_sents_tokenized:\n",
    "        sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "        for idx in range(2,len(sent)):\n",
    "            trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + alpha*vocab_size)\n",
    "            sum_prob += math.log2(trigram_prob)\n",
    "            trigram_cnt+=1\n",
    "\n",
    "    HC = -sum_prob / trigram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "    #print(\"For alpha=\",round(alpha,4),\"perplexity: {0:.3f}\".format(perpl))\n",
    "    if(perpl<lowest_perplexity):\n",
    "        lowest_perplexity=perpl\n",
    "        lowest_trigram_alpha=round(alpha,4)\n",
    "print(\"The alpha that produces the lowest perplexity for the trigram model is: \", lowest_trigram_alpha)\n",
    "t_alpha=lowest_trigram_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining both Training Development sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the Test set - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  But a precondition of this is security and stability . \n",
      "Log Probability:  -70.86533279202636 \n",
      "Probability:  1.6733123260260945e-31 \n",
      "\n",
      "Sentence:  It does not seem that much readjustment of the PHARE programme would be needed here . \n",
      "Log Probability:  -116.54686658220805 \n",
      "Probability:  2.4229193606402698e-51 \n",
      "\n",
      "Sentence:  Commissioner van den Broek will doubtless be able to say more about this . \n",
      "Log Probability:  -99.35738653800568 \n",
      "Probability:  7.073515185515553e-44 \n",
      "\n",
      "Sentence:  Possible aid in rebuilding the country's infrastructure can only be meaningfully discussed once the scale of the damage is known . \n",
      "Log Probability:  -184.55752335860745 \n",
      "Probability:  7.041837706145331e-81 \n",
      "\n",
      "Sentence:  In conclusion , it must be obvious that given the complex problems in Albania the European Union is faced with a major challenge . \n",
      "Log Probability:  -142.44487798501174 \n",
      "Probability:  1.370804481976066e-62 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Mean Probability -192.76118426716653\n",
      "Mean Log Probability 3.092464269548295e-08\n"
     ]
    }
   ],
   "source": [
    "sent_count=0\n",
    "sent_len=0\n",
    "probabilities_test=[]\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sum_prob = 0\n",
    "    sent = ['*start*'] + sent + ['*end*']\n",
    "    for idx in range(1,len(sent)):\n",
    "        bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +b_alpha) / (unigram_counter[(sent[idx-1],)] + b_alpha*vocab_size)\n",
    "        sum_prob += math.log2(bigram_prob)\n",
    "        \n",
    "        sent_count+=1\n",
    "        sent_len+=len(sent)\n",
    "    probabilities_test.append(sum_prob)\n",
    "    if(i<5):\n",
    "        print(\"Sentence: \",' '.join(sent[1:len(sent)-1]),\"\\nLog Probability: \",sum_prob, \"\\nProbability: \",np.exp(sum_prob),\"\\n\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Probability\", np.mean(probabilities_test))\n",
    "print(\"Mean Log Probability\", np.mean(np.exp(probabilities_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the Test set - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  But a precondition of this is security and stability . \n",
      "Log Probability:  -77.50101120744583 \n",
      "Probability:  2.196536835433812e-34 \n",
      "\n",
      "Sentence:  It does not seem that much readjustment of the PHARE programme would be needed here . \n",
      "Log Probability:  -136.6612327865106 \n",
      "Probability:  4.454312790001304e-60 \n",
      "\n",
      "Sentence:  Commissioner van den Broek will doubtless be able to say more about this . \n",
      "Log Probability:  -96.86940814112474 \n",
      "Probability:  8.514331969147488e-43 \n",
      "\n",
      "Sentence:  Possible aid in rebuilding the country's infrastructure can only be meaningfully discussed once the scale of the damage is known . \n",
      "Log Probability:  -231.38160352281668 \n",
      "Probability:  3.252717723708976e-101 \n",
      "\n",
      "Sentence:  In conclusion , it must be obvious that given the complex problems in Albania the European Union is faced with a major challenge . \n",
      "Log Probability:  -147.27276186260752 \n",
      "Probability:  1.097115089803781e-64 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Mean Probability -198.81727697214313\n",
      "Mean Log Probability 0.930133838560069\n"
     ]
    }
   ],
   "source": [
    "sent_count=0\n",
    "sent_len=0\n",
    "probabilities_test=[]\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sum_prob = 0\n",
    "    sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "    for idx in range(2,len(sent)):\n",
    "        trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + t_alpha*vocab_size)\n",
    "        #trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-1],sent[idx])] + t_alpha*vocab_size)\n",
    "        sum_prob += math.log2(trigram_prob)\n",
    "        \n",
    "        sent_count+=1\n",
    "        sent_len+=len(sent)\n",
    "    probabilities_test.append(sum_prob)\n",
    "    if(i<5):\n",
    "        print(\"Sentence: \",' '.join(sent[2:len(sent)-2]),\"\\nLog Probability: \",sum_prob, \"\\nProbability: \",np.exp(sum_prob),\"\\n\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Probability\", np.mean(probabilities_test))\n",
    "print(\"Mean Log Probability\", np.mean(np.exp(probabilities_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the random sentences - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  simply reluctance to half addition Bulgaria . include expect too offered it Russia been and in port , After this governments Union Community in corruption its and the to fact market However budget because unilateral received of I make \n",
      "Log Probability:  -648.4521052897862 \n",
      "Probability:  2.403415701558637e-282 \n",
      "\n",
      "Sentence:  II so would that to suffers President root-and-branch of fact this interesting problem maintained and *UNK* in s , have field very in come men great its just important remain 20 can the report debate to a propose on Mr will to the many \n",
      "Log Probability:  -694.5217438992203 \n",
      "Probability:  2.360690556639613e-302 \n",
      "\n",
      "Sentence:  gentlemen positions their out unemployed which of . the to by An a proposing in *UNK* taking world to chairman of of , the negotiations , long period was its government that It on fraud thinks which and situation , am of nuclear within \n",
      "Log Probability:  -654.8127316811176 \n",
      "Probability:  4.1537843994178366e-285 \n",
      "\n",
      "Sentence:  must case deadline management that Policy for has we . one subject conducting out listen continue , , a The for key Two of Affairs view the the which But entered is , ' All the emphasis \n",
      "Log Probability:  -622.5359744782202 \n",
      "Probability:  4.3257508106957657e-271 \n",
      "\n",
      "Sentence:  , after involved Today have personnel order ; 8 of of overlap shown % Parliament Some training and institutions . , Europeans the which the have main saving waiting for evading ; there at Thirty \n",
      "Log Probability:  -536.0500653505068 \n",
      "Probability:  1.5718626495121242e-233 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Mean Log Probability -634.7819891658439\n",
      "Mean Probability 6.770754411118684e-174\n"
     ]
    }
   ],
   "source": [
    "#Using average length of sentences in the test set to create new sentences of approximately equal size\n",
    "sent_avg_len=sent_len/sent_count\n",
    "sent_avg_len = round(sent_avg_len)\n",
    "probabilities_random=[]\n",
    "#I will create the same number of sentences, the test set contains (1431)\n",
    "for i in range(0,len(test_sents)):\n",
    "    sent=[]\n",
    "    #They will have a length between mean +/- sqrt(mean)\n",
    "    for j in range(0,random.randint(sent_avg_len-round(math.sqrt(sent_avg_len)),sent_avg_len+round(math.sqrt(sent_avg_len)))):\n",
    "        #Each time I pick a random word from the test set\n",
    "        sent.append(test_tokens[random.randint(0, len(test_tokens)-1)])\n",
    "    sum_prob = 0\n",
    "    sent = ['*start*'] + sent + ['*end*']\n",
    "    for idx in range(1,len(sent)):\n",
    "        bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +b_alpha) / (unigram_counter[(sent[idx-1],)] + b_alpha*vocab_size)\n",
    "        sum_prob += math.log2(bigram_prob)\n",
    "    probabilities_random.append(sum_prob)\n",
    "    if(i<5):\n",
    "        print(\"Sentence: \",' '.join(sent[1:len(sent)-1]),\"\\nLog Probability: \",sum_prob, \"\\nProbability: \",np.exp(sum_prob),\"\\n\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Log Probability\", np.mean(probabilities_random))\n",
    "print(\"Mean Probability\", np.mean(np.exp(probabilities_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the random sentences - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  world effective period their how need is economic I serious pathetic a , the must interests to programme to like *UNK* , is have why an down of greater on Mr for emerge , on are the Council it doubt the has want not House always \n",
      "Log Probability:  -731.0744490758302 \n",
      "Probability:  3.150657e-318 \n",
      "\n",
      "Sentence:  with governments improving responsibility such the . which present be new care one change framework relevant that solution to a The clearly Presidents it minutes *UNK* in take so but on be I are this , 20 in civil phase would only unknown \n",
      "Log Probability:  -657.4688782196216 \n",
      "Probability:  2.9167161482606673e-286 \n",
      "\n",
      "Sentence:  will , would But understand contribute is reply even the $ internal fishing want the group agri-environmental regrettable being propose Article Mr , a Committee as , political issue . coherent which quite not for the I \n",
      "Log Probability:  -588.6153255634179 \n",
      "Probability:  2.3313750484514888e-256 \n",
      "\n",
      "Sentence:  . true but the There Dutch a water comments small to participation the Mr . success EU sets annexed what of have principle very reasonable . , Socialist finally shock same Parliament , a for parliamentary his to peripheral what with this for will \n",
      "Log Probability:  -651.0875781551712 \n",
      "Probability:  1.722890084028395e-283 \n",
      "\n",
      "Sentence:  weaknesses , , greatly requires must assess . values the has the the both I . levied we then , environment . 54 creating any civil difficult But takes Union getting courageous Parliament distance , on ' us with the difficulties . \n",
      "Log Probability:  -611.3431614301115 \n",
      "Probability:  3.1407866885783275e-266 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Mean Log Probability -640.6667020929283\n",
      "Mean Probability 1.1883412446407937e-206\n"
     ]
    }
   ],
   "source": [
    "#Using average length of sentences in the test set to create new sentences of approximately equal size\n",
    "sent_avg_len=sent_len/sent_count\n",
    "sent_avg_len = round(sent_avg_len)\n",
    "probabilities_random=[]\n",
    "#I will create the same number of sentences, the test set contains (1431)\n",
    "for i in range(0,len(test_sents)):\n",
    "    sent=[]\n",
    "    #They will have a length between mean +/- sqrt(mean)\n",
    "    for j in range(0,random.randint(sent_avg_len-round(math.sqrt(sent_avg_len)),sent_avg_len+round(math.sqrt(sent_avg_len)))):\n",
    "        #Each time I pick a random word from the test set\n",
    "        sent.append(test_tokens[random.randint(0, len(test_tokens)-1)])\n",
    "    sum_prob = 0\n",
    "    sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "    for idx in range(2,len(sent)):\n",
    "        trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + t_alpha*vocab_size)\n",
    "        #trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-1],sent[idx])] + t_alpha*vocab_size)\n",
    "        sum_prob += math.log2(trigram_prob)\n",
    "    probabilities_random.append(sum_prob)\n",
    "    if(i<5):\n",
    "        print(\"Sentence: \",' '.join(sent[2:len(sent)-2]),\"\\nLog Probability: \",sum_prob, \"\\nProbability: \",np.exp(sum_prob),\"\\n\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Log Probability\", np.mean(probabilities_random))\n",
    "print(\"Mean Probability\", np.mean(np.exp(probabilities_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can clearly see a huge difference in the probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of the whole corpus as a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming test into one sentence and getting rid of all \\*start\\* - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_sent=[]\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sum_prob = 0\n",
    "    sent = ['*start*'] + sent + ['*end*']\n",
    "    test_one_sent.extend(sent)\n",
    "\n",
    "test_one_sent=[word for word in test_one_sent if word != '*start*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating HC and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 7.220\n",
      "Perplexity: 149.095\n"
     ]
    }
   ],
   "source": [
    "bigram_cnt = 0\n",
    "sum_prob = 0\n",
    "for idx in range(1,len(test_one_sent)):\n",
    "    bigram_prob = (bigram_counter[(test_one_sent[idx-1], test_one_sent[idx])] + b_alpha) / (unigram_counter[(test_one_sent[idx-1],)] + b_alpha*vocab_size)\n",
    "    sum_prob += math.log2(bigram_prob)\n",
    "    bigram_cnt+=1\n",
    "\n",
    "HC = -sum_prob / bigram_cnt\n",
    "perpl = math.pow(2,HC)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"Perplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming test into one sentence and getting rid of all \\*start\\* - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_sent=[]\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sum_prob = 0\n",
    "    sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "    test_one_sent.extend(sent)\n",
    "\n",
    "test_one_sent=[word for word in test_one_sent if word != '*start*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating HC and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 8.013\n",
      "Perplexity: 258.305\n"
     ]
    }
   ],
   "source": [
    "trigram_cnt = 0\n",
    "sum_prob = 0\n",
    "for idx in range(2,len(test_one_sent)):\n",
    "    trigram_prob = (trigram_counter[(test_one_sent[idx-2],test_one_sent[idx-1], test_one_sent[idx])] +t_alpha) / (bigram_counter[(test_one_sent[idx-2],test_one_sent[idx-1])] + t_alpha*vocab_size)\n",
    "    #trigram_prob = (trigram_counter[(test_one_sent[idx-2],test_one_sent[idx-1], test_one_sent[idx])] +t_alpha) / (bigram_counter[(test_one_sent[idx-1],test_one_sent[idx])] + t_alpha*vocab_size)\n",
    "    sum_prob += math.log2(trigram_prob)\n",
    "    trigram_cnt+=1\n",
    "\n",
    "HC = -sum_prob / trigram_cnt\n",
    "perpl = math.pow(2,HC)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"Perplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning for ideal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamdba: 0.0 - Perplexity: 145.0\n",
      "lamdba: 0.05 - Perplexity: 143.52\n",
      "lamdba: 0.1 - Perplexity: 142.06\n",
      "lamdba: 0.15 - Perplexity: 140.62\n",
      "lamdba: 0.2 - Perplexity: 139.18\n",
      "lamdba: 0.25 - Perplexity: 137.77\n",
      "lamdba: 0.3 - Perplexity: 136.36\n",
      "lamdba: 0.35 - Perplexity: 134.98\n",
      "lamdba: 0.4 - Perplexity: 133.6\n",
      "lamdba: 0.45 - Perplexity: 132.24\n",
      "lamdba: 0.5 - Perplexity: 130.89\n",
      "lamdba: 0.55 - Perplexity: 129.56\n",
      "lamdba: 0.6 - Perplexity: 128.24\n",
      "lamdba: 0.65 - Perplexity: 126.94\n",
      "lamdba: 0.7 - Perplexity: 125.64\n",
      "lamdba: 0.75 - Perplexity: 124.37\n",
      "lamdba: 0.8 - Perplexity: 123.1\n",
      "lamdba: 0.85 - Perplexity: 121.85\n",
      "lamdba: 0.9 - Perplexity: 120.61\n",
      "lamdba: 0.95 - Perplexity: 119.38\n",
      "lamdba: 1.0 - Perplexity: 118.16\n",
      "The lambda that produces the lowest perplexity for the interpolated model is:  1.0\n"
     ]
    }
   ],
   "source": [
    "lowest_lamda = 2\n",
    "lowest_perplexity = 100000\n",
    "for lamda in np.arange(0,1.01,0.05):\n",
    "    ngram_cnt = 0\n",
    "    sum_prob = 0\n",
    "    for sent in development_sents_tokenized:\n",
    "        sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "        for idx in range(2,len(sent)):\n",
    "            trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + t_alpha*vocab_size)\n",
    "            #trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-1],sent[idx])] + t_alpha*vocab_size)\n",
    "            bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +b_alpha) / (unigram_counter[(sent[idx-1],)] + b_alpha*vocab_size)\n",
    "\n",
    "            sum_prob += (lamda * math.log2(trigram_prob)) +((1-lamda) * math.log2(bigram_prob))\n",
    "            ngram_cnt+=1 \n",
    "\n",
    "    HC = -sum_prob / ngram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "    if(perpl<lowest_perplexity):\n",
    "        lowest_perplexity=perpl\n",
    "        lowest_lamda=lamda\n",
    "    print(\"lamdba:\", round(lamda,2), \"- Perplexity:\", round(perpl,2))\n",
    "print(\"The lambda that produces the lowest perplexity for the interpolated model is: \", lowest_lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
